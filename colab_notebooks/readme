# Overview
This project is part of a machine learning assignment focusing on decision trees and ensemble methods. By implementing these algorithms from scratch, we gain a clear understanding of their core principles, including:

How decision trees use impurity measures (e.g., Gini, entropy) to find optimal splits.
The concept of bootstrapping and random feature selection in random forests.
How AdaBoost iteratively adjusts sample weights to focus on harder cases.
The logic behind gradient boosting, where models are added sequentially to correct residual errors of previous models.
We then compare our from-scratch implementations with well-known libraries and frameworks, giving insight into their performance, features, and ease of use.

Implemented Algorithms
Decision Tree From Scratch
Implements ID3/CART-like logic for splitting nodes based on Gini or entropy.
Provides stopping criteria (max depth, min samples, etc.).
Demonstrates training and prediction on a simple dataset.
Random Forest From Scratch
Uses multiple decision trees trained on bootstrap samples of the data.
Randomly selects a subset of features for splitting at each node.
Aggregates predictions by majority vote (classification) or averaging (regression).
Showcases improved robustness and generalization over a single decision tree.
AdaBoost From Scratch
Trains a series of weak learners (e.g., decision stumps) on weighted samples.
Updates sample weights after each weak learner is added, emphasizing harder cases.
Combines weak learners into a strong classifier with improved accuracy.
Gradient Boosting (GBM) From Scratch
Begins with an initial model (often the mean prediction for regression).
Iteratively fits new models to the current residuals.
Uses a learning rate to gradually improve the modelâ€™s predictive power.
Demonstrates a flexible and powerful ensemble technique for both classification and regression.
Demonstrations
Classification with Decision Trees and Ensembles
Evaluate accuracy on a known classification dataset (e.g., Iris, Wine).
Compare the performance of a single decision tree with random forests, AdaBoost, and a GBM.
Regression with Ensemble Methods
Use a synthetic or real-world regression dataset.
Show how gradient boosting reduces residual errors over iterations.
Compare single-tree predictions to GBM predictions to highlight performance gains.
Advanced GBM Frameworks
After gaining an in-depth understanding of the fundamentals, we leverage well-known libraries to:

XGBoost, LightGBM, and CatBoost for Classification
Train and compare these frameworks on a classification task.
Explore feature importances, training speed, and accuracy.
XGBoost, LightGBM, and CatBoost for Regression
Apply these frameworks to a regression dataset.
Evaluate predictions against ground truth and analyze errors.
Gradient Boosting Ranking Techniques
Optional: Show how to use LightGBM or XGBoost for ranking tasks if a suitable dataset is provided.
Discuss parameter configurations for ranking and relevant evaluation metrics.
Getting Started
Prerequisites
Python 3.7+
Jupyter Notebook or Google Colab environment
